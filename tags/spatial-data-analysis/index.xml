<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spatial Data Analysis on Vincent Zhang</title>
    <link>/tags/spatial-data-analysis/index.xml</link>
    <description>Recent content in Spatial Data Analysis on Vincent Zhang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2016 Vincent Zhang</copyright>
    <atom:link href="/tags/spatial-data-analysis/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Modeling Groundwater Levels</title>
      <link>/projects/GWL_Modeling/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/GWL_Modeling/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#goals&#34;&gt;Goals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayesian-spatial-temporal-modeling&#34;&gt;Bayesian Spatial Temporal Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-ideas&#34;&gt;Other ideas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Groundwater monitoring and management is an essential issue in California because groundwater has tremendous values as a component of the hydrologic cycle for the environment and argriculture. This quarter I’m working at the Center for Water-Electricity Efficiency on campus for an interesting project about modeling groundwater levels. We aim to research on groundwater basins to support decisions around water reliability and resource sustainability by better understanding the groundwater levels change with respects to time and space. Through some literature research, spatial-temporal analysis and modeling is suitable for this project.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The data is provided from the Center for Water-Electricity Efficiency, which is also public available from the &lt;a href=&#34;http://www.water.ca.gov/groundwater/data_and_monitoring/index.cfm&#34;&gt;California Department of Water Resources&lt;/a&gt;. Besides, as we know that groundwater is very related to argriculture usage, we are specially interested in some probably useful variables to predict the ground water levels. Thus, we incorporate the evapotranspiration(ETo) data from the &lt;a href=&#34;http://wwwcimis.water.ca.gov/SpatialData.aspx&#34;&gt;California Irrigation Management Information System (CIMIS)&lt;/a&gt; which currently manages over 145 active weather stations throughout the state. ETo data is often used as a proxy metric to indicate the demand of argriculture water usage. Most of the CIMIS stations produce estimates of reference evapotranspiration (ETo) for the station location and their immediate surroundings, often in agricultural areas. In the end, we had to clean and process the data sources to merge the well information, monthly ETo data and the groundwater measurement data. Note that we focused on the San Joquain Valley as our first investigation, since the spatial-temporal analysis and modeling tends to be more robust and sound in the relatively same area. We used the data from 2011 to 2016 as the training set to train our model and the data of 2017 as the test set for model evaluation. Outlying cases had been removed for robustness concern in modeling. The initial data cleaning script can be found &lt;a href=&#34;https://github.com/jpzhangvincent/GroundwaterLevelSpatialDataProject/blob/master/Analysis/Data_Cleaning.R&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Goals&lt;/h3&gt;
&lt;p&gt;We know that some well locations track significantly more records of groundwater levels while other locations have missing values problems. Surface interpolation is common in spatial data analysis. On the other hand, forcasting on the time series of groundwater levels at a certain location would be also very useful. Thus, the goals of spatial-temporal modeling are two-folds: - Interpolate the missing values of groundwater levels at well locations - Forecast groundwater level at a certain time and location&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(reshape2)
library(ggplot2)
library(ggthemes)
library(viridis)
library(zoo)
library(spTimer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sj_gwl = read.csv(&amp;quot;sj_gwl_cleaned.csv&amp;quot;)
eto_data = read.csv(&amp;quot;ETo_cleaned.csv&amp;quot;)
all_data &amp;lt;- merge(sj_gwl, eto_data,  by = c(&amp;quot;Year&amp;quot;, &amp;quot;Month&amp;quot;))
cat(&amp;#39;The number of well location in the San Joaquin Valley: &amp;#39;, n_distinct(sj_gwl$LATITUDE, sj_gwl$LONGITUDE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The number of well location in the San Joaquin Valley:  436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we want to have a sense about how many missing records for a well location on a Yearly basis. Idealy, we hope each location has 12 measurements each year. From the following table and the heatmap, we clearly see that there is a significant problem of missing sensor measurements in the monthly data and only a few wells keep track the groundwater levels throughout the year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sj_gwl %&amp;gt;% group_by(CASGEM_STATION_ID, Year) %&amp;gt;% summarise(num_records = n()) %&amp;gt;% 
  reshape2::dcast(CASGEM_STATION_ID ~ Year, value.var = &amp;quot;num_records&amp;quot;) %&amp;gt;% 
  mutate(total_records = rowSums(.[2:7],na.rm = TRUE)) %&amp;gt;% arrange(desc(total_records)) %&amp;gt;% head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    CASGEM_STATION_ID 2011 2012 2013 2014 2015 2016 total_records
## 1               2774   11   10   12   10   11   11            65
## 2               6911   12   10   12   10    8    8            60
## 3              27637   12   10   12   10    8    8            60
## 4               3738   12    9    9    9   10   10            59
## 5              28548   12    7   10    9    9   10            57
## 6               3437   12    6    7    9   11   11            56
## 7               3207    8    9   11   10    9    7            54
## 8               3485    9    7   11    7    7   10            51
## 9               3948    8    7   10    6    8   10            49
## 10             26720   12   10   10    4    7    6            49
## 11             26749   NA    8    9    8   10    8            43
## 12             37917   12    7    4    8    5    5            41
## 13             49530   12   12    3   NA   NA   NA            27
## 14             49531   12   10   NA   NA    1   NA            23
## 15             49533   12   10   NA   NA    1   NA            23
## 16             49529   12   10   NA   NA   NA   NA            22
## 17             49527   12    9   NA   NA   NA   NA            21
## 18              3281   NA   NA   NA    5    8    7            20
## 19              6760    3    3    4    3    3    4            20
## 20              3129    1    3    4    2    2    4            16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_byyear &amp;lt;- sj_gwl %&amp;gt;% group_by(CASGEM_STATION_ID, Year) %&amp;gt;% summarise(num_records = n()) %&amp;gt;% 
  reshape2::dcast(CASGEM_STATION_ID ~ Year, value.var = &amp;quot;num_records&amp;quot;) %&amp;gt;% 
  mutate(total_records = rowSums(.[2:7],na.rm = TRUE)) 
count_byyear[is.na(count_byyear)] &amp;lt;- 0

newcount_byyear &amp;lt;- count_byyear %&amp;gt;% select(-total_records) %&amp;gt;% reshape2::melt(id = c(&amp;quot;CASGEM_STATION_ID&amp;quot;))
newcount_byyear$CASGEM_STATION_ID &amp;lt;- as.character(count_byyear$CASGEM_STATION_ID)
newcount_byyear$variable &amp;lt;- as.character(newcount_byyear$variable)
ggplot(newcount_byyear, aes(x= variable, y=CASGEM_STATION_ID)) + geom_tile(aes(fill = value)) + 
  scale_fill_viridis(name=&amp;quot;# Records&amp;quot;) +
  theme(axis.ticks.y=element_blank(), axis.text.y=element_blank()) +
  labs(x= &amp;quot;Year&amp;quot;, y= &amp;quot;Wells&amp;quot;, title=&amp;quot;Number of Measurements per year &amp;amp; well&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the ETo and groundwater levels are time dependent and possibly have correlation with each other, although we just made one example to illustrate our “guess” here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data$MEASUREMENT_DATE &amp;lt;- as.yearmon(paste(all_data$Year, all_data$Month), &amp;quot;%Y %m&amp;quot;)
ggplot(all_data[all_data$CASGEM_STATION_ID==2774,], aes(x = MEASUREMENT_DATE, y = ETO_avg)) + geom_line() +
  ggtitle(&amp;#39;ETo change over time for the well location 2774 &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(all_data[all_data$CASGEM_STATION_ID==2774,], aes(x = MEASUREMENT_DATE, y = Sol_Rad_avg)) + geom_line() +
  ggtitle(&amp;#39;Average Solar Radiation change for the well location 2774 &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(all_data[all_data$CASGEM_STATION_ID==2774,], aes(x = MEASUREMENT_DATE, y = WSEL_calc)) + geom_line() +
  ggtitle(&amp;#39;Groundwater Levels change for the well location 2774 &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s also visualize the well locations on a map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)
map &amp;lt;- get_map(location = &amp;#39;San Joaquin&amp;#39;, zoom =10, filename = &amp;#39;gwl_map.png&amp;#39;)
ggmap(map) +
  stat_density2d(aes(x = LONGITUDE, y = LATITUDE, fill = ..level.., alpha= ..level..), 
                 data = sj_gwl, size=2, bins = 4, geom = &amp;quot;polygon&amp;quot;) +
  facet_wrap(~ Year) + guides(alpha=FALSE, fill=guide_legend(title =&amp;quot;Measurements Density&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since response variable groundwater levels is calculated by adding Ground Surface Elevation to either the Water Surface reading or the negative Reference Point reading, the &lt;code&gt;WSEL&lt;/code&gt; can be negative. We would consider to properly transform the response variable for the purposes of normality and variance stabilizing in modeling. We can double check the distribution of the transformed variable. We use log transformation here while other box-cox transformations can be ultilized.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data$WSEL_transformed &amp;lt;- log(all_data$WSEL_calc + (1-min(all_data$WSEL_calc)))
hist(all_data$WSEL_calc, breaks = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(all_data$WSEL_transformed, breaks = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-9-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-spatial-temporal-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian Spatial Temporal Modeling&lt;/h2&gt;
&lt;p&gt;The nature of the given data has two components - space(latitude and longitude) and time. Studies in literatures show that spatial and temporal effects are important for modeling sensor data from monitoring stations. Spatio-temporal models would allow us to leverage the inference and predictive power of spatial and temporal effects. In our problems of modeling groundwater levels, we can think that the values are generated by an underlying process of interactions between space and time effects. And this underlying process is determined by unknown parameters with certain statistical properties. This hierarchical structure is well-suited to model in a Bayesian framework that we can incorporate the prior information and then use Bayesian updating to correct our beliefs in our parameters.&lt;/p&gt;
&lt;p&gt;“This kind of hierarchical structure allows partial pooling so that external data can be included in a model even if these external data share only some characteristics with the current data being modeled.” quoted from this &lt;a href=&#34;http://andrewgelman.com/2015/05/19/bayesian-inference-the-advantages-and-the-risks/&#34;&gt;article&lt;/a&gt; summarizes the advantages and risk of using bayesian inference. For example, in our case, we have missing values at different time stamps or locations. The partial pooling would use the relevance between different time and nearby neighbour locations to make inferences about the missing values. The R package &lt;code&gt;spTimer&lt;/code&gt; has a great implementation of Bayesian Spatial-Temporal model as it compares to other methods in its &lt;a href=&#34;https://www.jstatsoft.org/article/view/v063i15/v63i15.pdf&#34;&gt;document&lt;/a&gt;. However, a little downside of this package is that it requires special format of input. We have to make sure each location has the same time scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sj_train &amp;lt;- sj_gwl %&amp;gt;% filter(Year &amp;lt;= 2015)
sj_train$WSEL_transformed &amp;lt;- log(sj_train$WSEL_calc + (1-min(sj_gwl$WSEL_calc)))
sj_test &amp;lt;- sj_gwl %&amp;gt;% filter(Year &amp;gt; 2015)
sj_test$WSEL_transformed &amp;lt;- log(sj_test$WSEL_calc + (1-min(sj_gwl$WSEL_calc)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To apply the B.S.T Model in SpTimer package, we need to carefully remove the locations too close with each others, which is probably due to some computation issues.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;number of wells in San Joaquin Valley:&amp;quot;, n_distinct(sj_train$LATITUDE, sj_train$LONGITUDE), &amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of wells in San Joaquin Valley: 430&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loc_df &amp;lt;- as.matrix(unique(sj_train[,c(&amp;quot;LONGITUDE&amp;quot;,&amp;quot;LATITUDE&amp;quot;)]))
fdm &amp;lt;- spT.geodist(loc_df[,1],loc_df[,2])
diag(fdm)&amp;lt;-NA
fdm&amp;lt;-cbind(c(fdm),1:dim(fdm)[[2]],sort(rep(1:dim(fdm)[[1]],dim(fdm)[[2]])))
fdm&amp;lt;-fdm[!is.na(fdm[,1]),]
tol &amp;lt;- 0.05
fdmis&amp;lt;-fdm[fdm[,1] &amp;lt; tol,] #sites too close 
site_Todelete = unique(c(fdmis[,2],fdmis[,3]))
cat(&amp;quot;number of well locations being too close with each other: &amp;quot;,length(site_Todelete),&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of well locations being too close with each other:  22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loc_df = as.data.frame(loc_df)
loc_df$site_id = 1:dim(loc_df)[1]

nloc_df = loc_df %&amp;gt;% filter(!site_id %in% site_Todelete)
sj_train_cleaned &amp;lt;- left_join(nloc_df, sj_train) %&amp;gt;% group_by(LONGITUDE,LATITUDE, Year, Month) %&amp;gt;% 
  summarise(WSEL_transformed = mean(WSEL_transformed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;LONGITUDE&amp;quot;, &amp;quot;LATITUDE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Transform the input format such that each location has the same time units.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sj_train_cleaned$place &amp;lt;- paste(sj_train_cleaned$LONGITUDE,sj_train_cleaned$LATITUDE,sep=&amp;quot;_&amp;quot;)
N &amp;lt;- n_distinct(sj_train_cleaned$LATITUDE, sj_train_cleaned$LONGITUDE)
Y &amp;lt;- n_distinct(sj_train_cleaned$Year) 
tmp_data &amp;lt;- data.frame(matrix(NA, N*Y*12, 3))

tmp_data[,1] &amp;lt;- as.character(rep(unique(sj_train_cleaned$place), each = Y*12))
tmp_data[,2] &amp;lt;- as.numeric(rep(2011:2015,each = 12))
tmp_data[,3] &amp;lt;- as.numeric(1:12)
colnames(tmp_data) &amp;lt;- c(&amp;quot;place&amp;quot;, &amp;quot;Year&amp;quot;, &amp;quot;Month&amp;quot;)

tmp_sj_train_cleaned &amp;lt;- left_join(tmp_data, sj_train_cleaned, by = c(&amp;quot;place&amp;quot;, &amp;quot;Year&amp;quot;, &amp;quot;Month&amp;quot;))
train_data &amp;lt;- left_join(tmp_sj_train_cleaned, eto_data, by = c(&amp;quot;Year&amp;quot;, &amp;quot;Month&amp;quot;))#merge the eto data
#nall_data = merge(all_data,ntest,by =c(&amp;quot;place&amp;quot;, &amp;quot;Year&amp;quot;, &amp;quot;Season&amp;quot;),)
train_data$LONGITUDE = sapply(train_data[,&amp;quot;place&amp;quot;],function(x) as.numeric(strsplit(x, split=&amp;#39;_&amp;#39;)[[1]][1]))
train_data$LATITUDE = sapply(train_data[,&amp;quot;place&amp;quot;],function(x) as.numeric(strsplit(x, split=&amp;#39;_&amp;#39;)[[1]][2]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24480     8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(train_data$WSEL_transformed))/dim(train_data)[1] #33%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8978758&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From my trials and errors, I found a lot of missing values in the training data would result in the unstatability of computation(failure to run). Thus, we could rather choose the locations without missing too much data and reduce the computation complexity. Later, we could use the fitted model to predict the missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sj_gwl$place &amp;lt;- paste(sj_gwl$LONGITUDE,sj_gwl$LATITUDE,sep=&amp;quot;_&amp;quot;)
target_locs &amp;lt;- sj_gwl %&amp;gt;% group_by(place, Year) %&amp;gt;% summarise(num_records = n()) %&amp;gt;% 
  reshape2::dcast(place ~ Year, value.var = &amp;quot;num_records&amp;quot;) %&amp;gt;% 
  mutate(total_records = rowSums(.[2:7],na.rm = TRUE)) %&amp;gt;% arrange(desc(total_records)) %&amp;gt;% 
  filter(total_records&amp;gt;20) %&amp;gt;% select(place)
target_locs &amp;lt;- target_locs$place
target_train_data &amp;lt;- train_data[train_data$place %in% target_locs, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We decided to fit the spatial-temporal AR model. The AR model can be expressed as the following: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Z_{lt} &amp;amp;= O_{lt} + \epsilon_{lt} \\
O_{lt} &amp;amp;= \rho O_{lt-1} X_{lt}\beta + \eta_{lt}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is the observed values. &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; is the expected values. &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; refers to independent random noise. &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; refers to the random spatio-temporal interaction effect following multivariate normal distribution mean 0 and a certain covariance matrix(function). &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; refers the predictor coefficient estimates. &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; symbols for each location and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; indicates each time stamp. This AR model has an addition term &lt;span class=&#34;math inline&#34;&gt;\(\rho O_{lt-1}\)&lt;/span&gt; to introduce the time dependency structure like time series. The underlying unknown parameters are &lt;span class=&#34;math inline&#34;&gt;\(\theta=( \beta, \sigma_{\epsilon}, \sigma_{\eta}, \phi, \upsilon, \mu_l , \sigma_l)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\((\mu_l , \sigma_l)\)&lt;/span&gt; controls the time series structure for each location and &lt;span class=&#34;math inline&#34;&gt;\((\phi, \upsilon)\)&lt;/span&gt; controls the rate of decay/variation of the correlation matrix as the distance of two locations increases. The &lt;code&gt;spT.priors&lt;/code&gt; function allows to specify the priors of the parameters. We would use the statistics from the The &lt;code&gt;spT.decay&lt;/code&gt; function is used to assign prior distribution for the spatial decay parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2017)
#mean.prior &amp;lt;- mean(train_data$WSEL_transformed, na.rm=T)
#var.prior &amp;lt;- var(train_data$WSEL_transformed, na.rm=T)
#priors &amp;lt;- spT.priors(model = &amp;quot;AR&amp;quot;, inv.var.prior = Gamm(2,1), beta.prior = Norm(mean.prior, 100))
gwl.arx &amp;lt;- spT.Gibbs(formula = WSEL_transformed ~ ETO_avg + Sol_Rad_avg, data = 
                       target_train_data,
                     model =&amp;quot;AR&amp;quot;, nItr=5000, nBurn=1000,
                     spatial.decay= spT.decay(distribution = Gamm(a = 2, b = 1), tuning = 0.08),
                     coords = ~LONGITUDE+LATITUDE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Output: AR models 
## ---------------------------------------------------------------
##  Sampled: 5000 of 5000, 100.00%.
##  Batch Acceptance Rate (phi): 27.95%
##  Checking Parameters: 
##    phi: 0.0696, rho: 0.6664, sig2eps: 0.1337, sig2eta: 0.0837
##    beta[1]: 1.5416   beta[2]: 1.0442   beta[3]: -0.0006
## ---------------------------------------------------------------
## ## 
## # nBurn =  1000 , Iterations =  5000 . 
## # Overall Acceptance Rate (phi) =  27.94 % 
## ## 
## ##
## # Elapsed time: 5.04 Sec.
## ##
## 
## # Model: AR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PMCC refers to predictive model choice criteria, which is commonly used as the criteria to compare and choose model. The lower, the better. The acceptance rate is suggested to be in the range between 15% - 40% from the reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model selection criteria
gwl.arx$PMCC &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Goodness.of.fit Penalty   PMCC
## values:           38.96   78.82 117.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the model fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spT.validation(target_train_data$WSEL_transformed[!is.na(target_train_data$WSEL_transformed)],                                      gwl.arx$fitted[,1][!is.na(target_train_data$WSEL_transformed)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ##
##  Mean Squared Error (MSE) 
##  Root Mean Squared Error (RMSE) 
##  Mean Absolute Error (MAE) 
##  Mean Absolute Percentage Error (MAPE) 
##  Bias (BIAS) 
##  Relative Bias (rBIAS) 
##  Relative Mean Separation (rMSEP)
## ##&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    MSE   RMSE    MAE   MAPE   BIAS  rBIAS  rMSEP 
## 0.0808 0.2843 0.2199 5.6177 0.0021 0.0005 0.3134&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the metrics, it seems the model fits reasonably well. From the residual plots, it is not very promising since it seems to have a linear trend and there are some outlying cases at the tails.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(gwl.arx, residuals=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-19-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can predict on the origin training dataset which has more locations. Note that the output from the &lt;code&gt;predict&lt;/code&gt; function is a list, the predictioned are generated from the posterior distributions. The samples are stored in the &lt;code&gt;pred.sample&lt;/code&gt; sublist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interpolation.ar &amp;lt;- predict(gwl.arx, newdata= train_data, 
                            newcoords=~LONGITUDE+LATITUDE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Prediction: AR models 
## #
## # Tolerance Limit: 2
## # There are some Prediction locations very close to the Fitted locations.
## #
## [1] &amp;quot;Distance: 0  Predicted location: 12  Fitted location: 1 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 13  Fitted location: 2 &amp;quot;
## [1] &amp;quot;Distance: 1.27  Predicted location: 14  Fitted location: 2 &amp;quot;
## [1] &amp;quot;Distance: 1.27  Predicted location: 13  Fitted location: 3 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 14  Fitted location: 3 &amp;quot;
## [1] &amp;quot;Distance: 1.78  Predicted location: 18  Fitted location: 3 &amp;quot;
## [1] &amp;quot;Distance: 1.78  Predicted location: 14  Fitted location: 4 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 18  Fitted location: 4 &amp;quot;
## [1] &amp;quot;Distance: 1.1  Predicted location: 20  Fitted location: 4 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 19  Fitted location: 5 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 27  Fitted location: 6 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 41  Fitted location: 7 &amp;quot;
## [1] &amp;quot;Distance: 0.54  Predicted location: 46  Fitted location: 7 &amp;quot;
## [1] &amp;quot;Distance: 1.85  Predicted location: 62  Fitted location: 8 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 64  Fitted location: 8 &amp;quot;
## [1] &amp;quot;Distance: 1.64  Predicted location: 65  Fitted location: 8 &amp;quot;
## [1] &amp;quot;Distance: 1.1  Predicted location: 87  Fitted location: 9 &amp;quot;
## [1] &amp;quot;Distance: 1.94  Predicted location: 92  Fitted location: 9 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 96  Fitted location: 9 &amp;quot;
## [1] &amp;quot;Distance: 1.75  Predicted location: 104  Fitted location: 9 &amp;quot;
## [1] &amp;quot;Distance: 1.52  Predicted location: 105  Fitted location: 9 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 226  Fitted location: 10 &amp;quot;
## [1] &amp;quot;Distance: 1.8  Predicted location: 248  Fitted location: 10 &amp;quot;
## [1] &amp;quot;Distance: 1.36  Predicted location: 233  Fitted location: 11 &amp;quot;
## [1] &amp;quot;Distance: 1.1  Predicted location: 237  Fitted location: 11 &amp;quot;
## [1] &amp;quot;Distance: 1  Predicted location: 249  Fitted location: 11 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 252  Fitted location: 11 &amp;quot;
## [1] &amp;quot;Distance: 1.51  Predicted location: 264  Fitted location: 11 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 258  Fitted location: 12 &amp;quot;
## [1] &amp;quot;Distance: 1.97  Predicted location: 274  Fitted location: 12 &amp;quot;
## [1] &amp;quot;Distance: 1.48  Predicted location: 260  Fitted location: 13 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 268  Fitted location: 13 &amp;quot;
## [1] &amp;quot;Distance: 1.99  Predicted location: 312  Fitted location: 14 &amp;quot;
## [1] &amp;quot;Distance: 1.62  Predicted location: 316  Fitted location: 14 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 327  Fitted location: 14 &amp;quot;
## [1] &amp;quot;Distance: 1.64  Predicted location: 331  Fitted location: 14 &amp;quot;
## [1] &amp;quot;Distance: 1.57  Predicted location: 332  Fitted location: 14 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 362  Fitted location: 15 &amp;quot;
## [1] &amp;quot;Distance: 2  Predicted location: 387  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 393  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 1.53  Predicted location: 395  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 1.42  Predicted location: 397  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 0.96  Predicted location: 400  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 1.42  Predicted location: 406  Fitted location: 16 &amp;quot;
## [1] &amp;quot;Distance: 0  Predicted location: 394  Fitted location: 17 &amp;quot;
## #
## # Romove the locations and run again. 
## #
## -------------------------------------------------
##   Sampled: 400 of 4000, 10.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 800 of 4000, 20.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 1200 of 4000, 30.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 1600 of 4000, 40.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 2000 of 4000, 50.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 2400 of 4000, 60.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 2800 of 4000, 70.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 3200 of 4000, 80.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 3600 of 4000, 90.00%
## -------------------------------------------------
## -------------------------------------------------
##   Sampled: 4000 of 4000, 100.00%
## -------------------------------------------------
## ## 
## # Predicted samples and summary statistics are given.
## # nBurn =  1000 . Iterations =  5000 . 
## ## 
## ##
## # Elapsed time: 7 Min. 11.83 Sec.
## ##&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;Mean&lt;/code&gt; sublist from the output as our predictions. The number of rows means the time range(in order) and the number of columns means the number of unique of locations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(interpolation.ar$Mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  60 408&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to rescale the prediction into the original scale to compare the fitting performance. We can observe that the model tends to underfit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spt_pred &amp;lt;- as.vector(matrix(interpolation.ar$Median, ncol = 1))
train_data$actual &amp;lt;- exp(train_data$WSEL_transformed) - (1-min(sj_gwl$WSEL_calc))
train_data$preds &amp;lt;- exp(spt_pred)-(1-min(sj_gwl$WSEL_calc)) 
plot(train_data$preds, train_data$actual)
lines(x = c(-100,100), y = c(-100,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we want to check out the model performance for a specific location?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_gwl &amp;lt;- function(long, lat, plot_data){
   plot_data$Date &amp;lt;- as.yearmon(paste(plot_data$Year, plot_data$Month), &amp;quot;%Y %m&amp;quot;)
   plot_data %&amp;gt;% filter(LONGITUDE==long, LATITUDE==lat) %&amp;gt;% 
      select(Date, actual, preds) %&amp;gt;%  melt(id=&amp;quot;Date&amp;quot;) %&amp;gt;% 
      ggplot(aes(x=Date,y= value,colour= variable,group=variable)) + 
        geom_line() + labs(x= &amp;quot;Date&amp;quot;, y= &amp;quot;Groundwater Levels&amp;quot;, title= paste(&amp;quot;Groundwater levels at &amp;quot;, long,&amp;#39;,&amp;#39;, lat))
         
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check one location that has relatively rich data points. It’s clear to see that the trend component of the time series is captured well in this case, while there is some bias between the actual values and prediction/interpolations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_gwl(-121.0765, 37.9331, train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;#####../content/projects/GWL_Modeling_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next steps, we need to come up with ways for bias correction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-ideas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other ideas&lt;/h2&gt;
&lt;p&gt;Being able to understand and predict groundwater level changes would give us insights like how to save energy and how to distribute water resource in a more efficient way. In the project, I communicated and learned from domain experts to better understand the context, proposed ways to collect/aggregate useful information and tried to improve the spatial temporal model. However, this kind of modeling problem is challenging because it’s related to the complex climate system. And there is still a lot of room to verify our hypothesis and improve our modeling. A few other ideas are worthwhile to try:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bias correction for outlier cases through bootsrapped estimates through close points&lt;/li&gt;
&lt;li&gt;Try generative additive model or machine learning approach&lt;/li&gt;
&lt;li&gt;We can consider whether we could incorporate more factors like weather or geographic features into our original model to improve predictive power.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    script.src  = &#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;;
    if (location.protocol !== &#34;file:&#34; &amp;&amp; /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, &#39;&#39;);
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;

&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
  </channel>
</rss>